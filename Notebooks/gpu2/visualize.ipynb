{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 07:27:21.203752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-30 07:27:21.220976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732919241.240953 3231851 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732919241.247011 3231851 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 07:27:21.269957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite cfg.model.RecognitionNetwork.keypoint_s3d.in_channel -> 79\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 259\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 242\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments/configs/TwoStream/phoenix-2014_keypoint.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Create demo instance\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m demo \u001b[38;5;241m=\u001b[39m \u001b[43mSignLanguageRecognitionDemo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Choose demo mode\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSign Language Recognition Demo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mSignLanguageRecognitionDemo.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2gloss \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgloss_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Load best checkpoint\u001b[39;00m\n\u001b[1;32m     38\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/SLRT/TwoStreamNetwork/modelling/model.py:128\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(cfg):\n\u001b[0;32m--> 128\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSignLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/SLRT/TwoStreamNetwork/modelling/model.py:11\u001b[0m, in \u001b[0;36mSignLanguageModel.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m \u001b[43mget_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m], cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m     model_cfg \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/SLRT/TwoStreamNetwork/utils/misc.py:90\u001b[0m, in \u001b[0;36mget_logger\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_logger\u001b[39m():\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogger\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import necessary project modules\n",
    "from modelling.model import build_model\n",
    "from dataset.Dataloader import build_dataloader\n",
    "from utils.misc import load_config\n",
    "\n",
    "class SignLanguageRecognitionDemo:\n",
    "    def __init__(self, config_path):\n",
    "        \"\"\"\n",
    "        Initialize the sign language recognition demo\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): Path to the configuration YAML file\n",
    "        \"\"\"\n",
    "        # Load configuration\n",
    "        self.cfg = load_config(config_path)\n",
    "        \n",
    "        # Load gloss dictionary\n",
    "        with open(self.cfg['model']['RecognitionNetwork']['GlossTokenizer']['gloss2id_file'], 'rb') as f:\n",
    "            self.gloss_dict = pickle.load(f)\n",
    "        self.id2gloss = {v: k for k, v in self.gloss_dict.items()}\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = build_model(self.cfg)\n",
    "        \n",
    "        # Load best checkpoint\n",
    "        checkpoint_path = os.path.join(self.cfg['training']['model_dir'], 'ckpts', 'best.ckpt')\n",
    "        state_dict = torch.load(checkpoint_path, map_location='cuda')\n",
    "        self.model.load_state_dict(state_dict['model_state'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize MediaPipe for keypoint extraction\n",
    "        self.mp_holistic = mp.solutions.holistic\n",
    "        self.holistic = self.mp_holistic.Holistic(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=1,\n",
    "            smooth_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Prepare video preprocessing\n",
    "        self.transform_cfg = self.cfg['data']['transform_cfg']\n",
    "    \n",
    "    def extract_keypoints(self, frame):\n",
    "        \"\"\"\n",
    "        Extract keypoints from a frame using MediaPipe\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): Input video frame\n",
    "        \n",
    "        Returns:\n",
    "            dict: Extracted keypoints\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame\n",
    "        results = self.holistic.process(rgb_frame)\n",
    "        \n",
    "        # Extract keypoints\n",
    "        keypoints = {\n",
    "            'pose': [],\n",
    "            'mouth_half': [],\n",
    "            'hand': [],\n",
    "            'face_others_1_3': []\n",
    "        }\n",
    "        \n",
    "        # Pose landmarks\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints['pose'].extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        \n",
    "        # Left and right hand landmarks\n",
    "        def process_hand(hand_landmarks):\n",
    "            hand_points = []\n",
    "            if hand_landmarks:\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    hand_points.extend([landmark.x, landmark.y, landmark.z])\n",
    "            return hand_points\n",
    "        \n",
    "        keypoints['hand'] = process_hand(results.left_hand_landmarks) + \\\n",
    "                             process_hand(results.right_hand_landmarks)\n",
    "        \n",
    "        # Face landmarks (using a subset)\n",
    "        if results.face_landmarks:\n",
    "            face_points = results.face_landmarks.landmark\n",
    "            # Select a subset of face landmarks (you might want to customize this)\n",
    "            face_subset = face_points[0:68]  # Example: first 68 landmarks\n",
    "            keypoints['face_others_1_3'] = [\n",
    "                [landmark.x, landmark.y, landmark.z] for landmark in face_subset\n",
    "            ]\n",
    "        \n",
    "        # Mouth landmarks (simplified)\n",
    "        if results.face_landmarks:\n",
    "            mouth_points = results.face_landmarks.landmark[0:20]  # Adjust as needed\n",
    "            keypoints['mouth_half'] = [\n",
    "                [landmark.x, landmark.y, landmark.z] for landmark in mouth_points\n",
    "            ]\n",
    "        \n",
    "        return keypoints\n",
    "    \n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Preprocess frame for model input\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): Input video frame\n",
    "        \n",
    "        Returns:\n",
    "            dict: Preprocessed inputs for the model\n",
    "        \"\"\"\n",
    "        # Resize frame\n",
    "        frame = cv2.resize(frame, (self.transform_cfg['img_size'], self.transform_cfg['img_size']))\n",
    "        \n",
    "        # Extract keypoints\n",
    "        keypoints = self.extract_keypoints(frame)\n",
    "        \n",
    "        # Convert frame to tensor\n",
    "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "        frame_tensor = frame_tensor.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Convert keypoints to tensor\n",
    "        keypoint_tensors = {k: torch.tensor(v).float().unsqueeze(0) for k, v in keypoints.items()}\n",
    "        \n",
    "        return {\n",
    "            'rgb': frame_tensor,\n",
    "            'keypoint': keypoint_tensors\n",
    "        }\n",
    "    \n",
    "    def recognize_sign(self, frame):\n",
    "        \"\"\"\n",
    "        Recognize sign language from a frame\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): Input video frame\n",
    "        \n",
    "        Returns:\n",
    "            str: Recognized gloss (sign)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Preprocess frame\n",
    "            inputs = self.preprocess_frame(frame)\n",
    "            \n",
    "            # Run model\n",
    "            output = self.model(is_train=False, **inputs)\n",
    "            \n",
    "            # Get top predictions\n",
    "            if 'recognition_logits' in output:\n",
    "                logits = output['recognition_logits']\n",
    "                top_k_values, top_k_indices = torch.topk(logits, k=5)\n",
    "                predictions = [self.id2gloss[idx.item()] for idx in top_k_indices[0]]\n",
    "                \n",
    "                return predictions\n",
    "        \n",
    "        return [\"No sign detected\"]\n",
    "    \n",
    "    def run_webcam_demo(self):\n",
    "        \"\"\"\n",
    "        Run real-time sign language recognition demo using webcam\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Flip frame horizontally for natural view\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Recognize sign\n",
    "            predictions = self.recognize_sign(frame)\n",
    "            \n",
    "            # Display predictions\n",
    "            for i, pred in enumerate(predictions):\n",
    "                cv2.putText(frame, f\"{i+1}. {pred}\", \n",
    "                            (10, 30 + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Sign Language Recognition', frame)\n",
    "            \n",
    "            # Exit on 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def recognize_video_file(self, video_path):\n",
    "        \"\"\"\n",
    "        Recognize signs in a video file\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Path to the input video file\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Recognize sign\n",
    "            predictions = self.recognize_sign(frame)\n",
    "            \n",
    "            # Display predictions\n",
    "            for i, pred in enumerate(predictions):\n",
    "                cv2.putText(frame, f\"{i+1}. {pred}\", \n",
    "                            (10, 30 + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Sign Language Recognition', frame)\n",
    "            \n",
    "            # Exit on 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Path to your configuration file\n",
    "    config_path = \"experiments/configs/TwoStream/phoenix-2014_keypoint.yaml\"\n",
    "    \n",
    "    # Create demo instance\n",
    "    demo = SignLanguageRecognitionDemo(config_path)\n",
    "    \n",
    "    # Choose demo mode\n",
    "    print(\"Sign Language Recognition Demo\")\n",
    "    print(\"1. Webcam Demo\")\n",
    "    print(\"2. Video File Demo\")\n",
    "    choice = input(\"Enter your choice (1/2): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        demo.run_webcam_demo()\n",
    "    elif choice == '2':\n",
    "        video_path = input(\"Enter the path to your video file: \")\n",
    "        demo.recognize_video_file(video_path)\n",
    "    else:\n",
    "        print(\"Invalid choice!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is accessible\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        print(\"Camera is accessible\")\n",
    "    else:\n",
    "        print(\"Failed to capture frame\")\n",
    "else:\n",
    "    print(\"Failed to open camera\")\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open camera\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(-1)\n",
    "if cap.isOpened():\n",
    "    print(\"Camera is accessible\")\n",
    "else:\n",
    "    print(\"Failed to open camera\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
