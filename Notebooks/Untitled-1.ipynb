{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file = \"/nas/Dataset/Phoenix/phoenix-2014-videos.zip\"\n",
    "extract_dir = \"extracted_videos\"  # Replace with your desired directory\n",
    "\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "  # Print list of files in the zip\n",
    "  print(\"Files in the zip:\")\n",
    "  for filename in zip_ref.namelist():\n",
    "    print(filename)\n",
    "\n",
    "  # Extract all files to the extract_dir\n",
    "  zip_ref.extractall(extract_dir)\n",
    "  print(f\"Videos extracted to: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_phoenix_split(split_path):\n",
    "    \"\"\"\n",
    "    Read a single Phoenix-2014 dataset split file from gzipped pickle format\n",
    "    \n",
    "    Args:\n",
    "        split_path (str): Path to the gzipped split file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing sample information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with gzip.open(split_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Successfully loaded split file: {len(data)} samples\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading split file {split_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_split_data(split_data):\n",
    "    \"\"\"\n",
    "    Analyze the content of a split dataset\n",
    "    \n",
    "    Args:\n",
    "        split_data (list): List of dictionaries containing sample information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics about the split\n",
    "    \"\"\"\n",
    "    if not split_data:\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'num_samples': len(split_data),\n",
    "        'fields_available': set().union(*[set(d.keys()) for d in split_data]),\n",
    "        'sample_lengths': [],\n",
    "        'glosses': set(),\n",
    "        'signers': set() if 'signer' in split_data[0] else None\n",
    "    }\n",
    "    \n",
    "    for sample in split_data:\n",
    "        if 'num_frames' in sample:\n",
    "            stats['sample_lengths'].append(sample['num_frames'])\n",
    "        if 'gloss' in sample:\n",
    "            stats['glosses'].add(sample['gloss'])\n",
    "        if 'signer' in sample:\n",
    "            stats['signers'].add(sample['signer'])\n",
    "    \n",
    "    if stats['sample_lengths']:\n",
    "        stats['avg_length'] = np.mean(stats['sample_lengths'])\n",
    "        stats['min_length'] = np.min(stats['sample_lengths'])\n",
    "        stats['max_length'] = np.max(stats['sample_lengths'])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def analyze_keypoints(keypoints):\n",
    "    \"\"\"\n",
    "    Analyze the keypoint data structure and content\n",
    "    \n",
    "    Args:\n",
    "        keypoints (dict): Dictionary containing keypoint data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics about the keypoints\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'num_samples': len(keypoints),\n",
    "        'sample_key_format': list(keypoints.keys())[0],\n",
    "    }\n",
    "    \n",
    "    # Analyze first sample\n",
    "    sample_data = keypoints[stats['sample_key_format']]\n",
    "    if 'keypoints' in sample_data:\n",
    "        kp = sample_data['keypoints']\n",
    "        stats.update({\n",
    "            'shape': kp.shape,\n",
    "            'num_frames': kp.shape[0],\n",
    "            'num_keypoints': kp.shape[1],\n",
    "            'dimensions': kp.shape[2],\n",
    "            'min_value': float(np.min(kp)),\n",
    "            'max_value': float(np.max(kp)),\n",
    "            'mean_value': float(np.mean(kp))\n",
    "        })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def load_phoenix_dataset(config):\n",
    "    \"\"\"\n",
    "    Load complete Phoenix dataset including splits and keypoints\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing paths\n",
    "            Required keys:\n",
    "            - train: path to train split file\n",
    "            - dev: path to dev split file\n",
    "            - test: path to test split file\n",
    "            - keypoint_file: path to keypoints pickle file\n",
    "            \n",
    "    Returns:\n",
    "        tuple: (splits_dict, keypoints_dict, analysis_dict)\n",
    "    \"\"\"\n",
    "    splits = {}\n",
    "    analysis = {'splits': {}}\n",
    "    \n",
    "    # Read each split file\n",
    "    for split_name in ['train', 'dev', 'test']:\n",
    "        split_path = config[split_name]\n",
    "        if os.path.exists(split_path):\n",
    "            splits[split_name] = read_phoenix_split(split_path)\n",
    "            analysis['splits'][split_name] = analyze_split_data(splits[split_name])\n",
    "        else:\n",
    "            print(f\"Warning: Split file not found at {split_path}\")\n",
    "    \n",
    "    # Read keypoints\n",
    "    try:\n",
    "        with open(config['keypoint_file'], 'rb') as f:\n",
    "            keypoints = pickle.load(f)\n",
    "        print(f\"Successfully loaded keypoints for {len(keypoints)} samples\")\n",
    "        analysis['keypoints'] = analyze_keypoints(keypoints)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading keypoint file: {str(e)}\")\n",
    "        keypoints = None\n",
    "    \n",
    "    return splits, keypoints, analysis\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"data/phoenix-2014\"  # Adjust this to your actual base path\n",
    "    config = {\n",
    "        'train': os.path.join(base_path, \"phoenix-2014.train\"),\n",
    "        'dev': os.path.join(base_path, \"phoenix-2014.dev\"),\n",
    "        'test': os.path.join(base_path, \"phoenix-2014.test\"),\n",
    "        'keypoint_file': '/nas/Dataset/Phoenix/phoenix-2014-keypoints.pkl'\n",
    "    }\n",
    "    \n",
    "    splits, keypoints, analysis = load_phoenix_dataset(config)\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(\"\\nDATASET ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split analysis\n",
    "    for split_name, stats in analysis['splits'].items():\n",
    "        if stats:\n",
    "            print(f\"\\n{split_name.upper()} Split Statistics:\")\n",
    "            print(f\"Number of samples: {stats['num_samples']}\")\n",
    "            print(f\"Available fields: {', '.join(sorted(stats['fields_available']))}\")\n",
    "            if stats.get('avg_length'):\n",
    "                print(f\"Average sequence length: {stats['avg_length']:.1f} frames\")\n",
    "                print(f\"Sequence length range: {stats['min_length']} - {stats['max_length']} frames\")\n",
    "            if stats['glosses']:\n",
    "                print(f\"Number of unique glosses: {len(stats['glosses'])}\")\n",
    "            if stats['signers']:\n",
    "                print(f\"Number of unique signers: {len(stats['signers'])}\")\n",
    "    \n",
    "    # Keypoint analysis\n",
    "    if 'keypoints' in analysis:\n",
    "        kp_stats = analysis['keypoints']\n",
    "        print(\"\\nKEYPOINT Statistics:\")\n",
    "        print(f\"Number of samples: {kp_stats['num_samples']}\")\n",
    "        print(f\"Data shape: {kp_stats['shape']} (frames, keypoints, coordinates)\")\n",
    "        print(f\"Value range: [{kp_stats['min_value']:.3f}, {kp_stats['max_value']:.3f}]\")\n",
    "        print(f\"Mean value: {kp_stats['mean_value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "def read_and_show_examples(split_path, num_examples=3):\n",
    "    \"\"\"\n",
    "    Read and display examples from a split file\n",
    "    \n",
    "    Args:\n",
    "        split_path (str): Path to the gzipped split file\n",
    "        num_examples (int): Number of examples to display\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with gzip.open(split_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\nLoaded {len(data)} samples from {os.path.basename(split_path)}\")\n",
    "        print(\"\\nFirst few examples:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, sample in enumerate(data[:num_examples]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(\"-\" * 40)\n",
    "            # Print each field in the sample\n",
    "            for key, value in sample.items():\n",
    "                if isinstance(value, (np.ndarray, list)) and len(str(value)) > 100:\n",
    "                    print(f\"{key}: {type(value)} with shape {np.array(value).shape}\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Print all available fields in the dataset\n",
    "        print(\"\\nAll available fields in the dataset:\")\n",
    "        print(set().union(*[set(d.keys()) for d in data]))\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading split file {split_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"data/phoenix-2014\"\n",
    "    splits = {\n",
    "        'train': os.path.join(base_path, \"phoenix-2014.train\"),\n",
    "        'dev': os.path.join(base_path, \"phoenix-2014.dev\"),\n",
    "        'test': os.path.join(base_path, \"phoenix-2014.test\")\n",
    "    }\n",
    "    \n",
    "    # Read and show examples from each split\n",
    "    for split_name, split_path in splits.items():\n",
    "        if os.path.exists(split_path):\n",
    "            print(f\"\\n{split_name.upper()} SPLIT:\")\n",
    "            data = read_and_show_examples(split_path)\n",
    "            \n",
    "            if data:\n",
    "                # Print some additional statistics\n",
    "                print(f\"\\nAdditional statistics for {split_name} split:\")\n",
    "                print(f\"Total number of samples: {len(data)}\")\n",
    "                if 'gloss' in data[0]:\n",
    "                    unique_glosses = set(d['gloss'] for d in data)\n",
    "                    print(f\"Number of unique glosses: {len(unique_glosses)}\")\n",
    "                if 'signer' in data[0]:\n",
    "                    unique_signers = set(d['signer'] for d in data)\n",
    "                    print(f\"Number of unique signers: {len(unique_signers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def read_split_file(filepath):\n",
    "    \"\"\"Simple function to read and display contents of a Phoenix split file\"\"\"\n",
    "    print(f\"\\nReading: {filepath}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Total samples: {len(data)}\")\n",
    "        print(\"\\nFirst 3 samples:\")\n",
    "        for i, sample in enumerate(data[:3]):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(sample)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Read each split file\n",
    "splits = [\n",
    "    \"data/phoenix-2014/phoenix-2014.train\",\n",
    "    \"data/phoenix-2014/phoenix-2014.dev\",\n",
    "    \"data/phoenix-2014/phoenix-2014.test\"\n",
    "]\n",
    "\n",
    "for split_file in splits:\n",
    "    read_split_file(split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
